{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lending Club Loan Default\n",
    "\n",
    "In this tutorial, we will go through a step-by-step workflow to determine loan deliquency. We will make predictions based only on the information available at the time the loan was issued.  The data for this exercise come from the public Lending Club data set, a description can be found [here](https://www.kaggle.com/pragyanbo/a-hitchhiker-s-guide-to-lending-club-loan-data/notebook).\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. Start the Sparkling Water cluster\n",
    "2. Import data\n",
    "3. Clean data\n",
    "4. Feature engineering\n",
    "5. Model training\n",
    "6. Examine model accuracy\n",
    "7. Interpret model\n",
    "8. Save and reuse model\n",
    "9. AutoML (optional)\n",
    "10. Stop the Sparkling Water cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 (of 10). Start the Sparkling Water cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by starting the Sparkling Water cluster and importing the data into our H2O cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate H2OContext on top of Spark\n",
    "from pysparkling import *\n",
    "hc = H2OContext.getOrCreate()\n",
    "\n",
    "import h2o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The method you use for starting and stopping an Sparkling Water cluster will depend on how H2O is installed and configured on your system. Regardless of how H2O is installed, if you start a cluster, you will need to ensure that it is shut down when you are done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 (of 10). Import data\n",
    "\n",
    "The data set we use below is a local copy of https://s3-us-west-2.amazonaws.com/h2o-tutorials/data/topics/lending/lending_club/LoanStats3a.csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "input_csv = \"/home/h2o/data/lending_club/LoanStats3a.csv\"\n",
    "\n",
    "if not os.path.exists(input_csv):\n",
    "    input_csv = \"https://s3-us-west-2.amazonaws.com/h2o-tutorials/data/topics/lending/lending_club/LoanStats3a.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View and Inspect the Data\n",
    "\n",
    "The loans data set is loaded directly into the H2O-3 cluster using the `h2o.import_file` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans = h2o.import_file(input_csv,\n",
    "                        col_types = {\"int_rate\":\"string\", \n",
    "                                     \"revol_util\":\"string\", \n",
    "                                     \"emp_length\":\"string\", \n",
    "                                     \"verification_status\":\"string\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `h2o.import_file` command loads the data directly into the H2O-3 cluster's memory, bypassing python. If one already has data in python, one can pass that object to a H2O Frame. The `h2o.import_file` approach is incredibly efficient, particularly for larg data sets.\n",
    "\n",
    "### Inspect the Data with H2O Flow\n",
    "\n",
    "Now is a good time to connect to H2O Flow. Although H2O Flow can be used for everything from loading data to building models to creating production code, we use it here for data investigation and H2O system monitoring.\n",
    "\n",
    "**Note**: the reported IP above, `http://localhost`, is the local IP within your particular cloud instance. To open H2O Flow in your own browser, copy the browser URL and\n",
    "\n",
    "**Aquarium**: \n",
    ">Replace \n",
    ">```\n",
    "http://{your URL}/jupyter/\n",
    ">``` \n",
    ">with \n",
    ">```\n",
    "http://{your URL}/h2o/\n",
    ">```\n",
    "\n",
    "**Otherwise**: \n",
    ">Replace\n",
    ">```\n",
    "http://{your URL}:8888\n",
    ">``` \n",
    ">with \n",
    ">```\n",
    "http://{your URL}:54321\n",
    ">```\n",
    "\n",
    "Some summary information on the data is provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    ">## Comment: Data preparation for modeling\n",
    ">\n",
    ">In this tutorial, we either rush through or perhaps skip steps that a modeler would typically spend a considerable amount of effort on. For example, we omit entirely exploratory data analysis. Additionally, the process of **_defining the problem_** is often iterative and takes a lot of thought and effort. We make the assumption here that this work has already been done by the modeler.\n",
    ">\n",
    ">In reality, the majority of a modeler's time is spent on problem definition and data cleaning/wrangling/munging. Our speed at going through these steps to demonstrate the use of H2O-3 in no way minimizes the importance of careful and thoughtful data preparation for model building. \n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 (of 10).  Clean data\n",
    "\n",
    "\n",
    "## Part 1. Defining the problem and creating the response variable\n",
    "\n",
    "The total number of loans in our data set is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unfiltered_loans = loans.dim[0]\n",
    "num_unfiltered_loans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are interested in loan default, we need to look at the `loan_status` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans[\"loan_status\"].table().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like many real data sources, `loan_status` is messy and contains multiple, somewhat overlapping, categories. Before modeling, we will need to clean this up by (a) removing loans that are still ongoing, and (b) simplifying the response column.\n",
    "\n",
    "### (a) Filter Loans\n",
    "\n",
    "In order to build a valid model, we have to remove loans that are still in process. They have `loan_status` like \"Current\" and \"In Grace Period\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ongoing_status = [\"Current\",\n",
    "                  \"In Grace Period\",\n",
    "                  \"Late (16-30 days)\",\n",
    "                  \"Late (31-120 days)\",\n",
    "                  \"Does not meet the credit policy.  Status:Current\",\n",
    "                  \"Does not meet the credit policy.  Status:In Grace Period\"\n",
    "                 ]\n",
    "loans = loans[~loans[\"loan_status\"].isin(ongoing_status)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After filtering out ongoing loans, we now have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filtered_loans = loans.dim[0]\n",
    "num_filtered_loans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loans whose final state is known, which means we filtered out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_loans_filtered_out = num_unfiltered_loans - num_filtered_loans\n",
    "num_loans_filtered_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loans. These loans are now summarized by `loan_status` as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans[\"loan_status\"].table().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Create Response Column\n",
    "\n",
    "Let's name our response column `bad_loan`, which will equal one if the loan was not completely paid off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fully_paid = [\"Fully Paid\",\n",
    "              \"Does not meet the credit policy.  Status:Fully Paid\"\n",
    "             ]\n",
    "loans[\"bad_loan\"] = ~(loans[\"loan_status\"].isin(fully_paid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next make the `bad_loan` column a factor so that we can build a classification model,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans[\"bad_loan\"] = loans[\"bad_loan\"].asfactor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentage of bad loans is given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_loan_dist = loans[\"bad_loan\"].table()\n",
    "bad_loan_dist[\"Percentage\"] = (100 * bad_loan_dist[\"Count\"] / loans.nrow).round()\n",
    "bad_loan_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Convert strings to numeric\n",
    "\n",
    "Consider the columns `int_rate`, `revol_util`, and `emp_length`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans[[\"int_rate\", \"revol_util\", \"emp_length\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `int_rate` and `revol_util` are inherently numeric but entered as percentages. Since they include a \"%\" sign, they are read in as strings. The solution for both of these columns is simple: strip the \"%\" sign and convert the strings to numeric.\n",
    "\n",
    "The `emp_length` column is only slightly more complex. Besides removing the \"year\" or \"years\" term, we have to deal with `< 1` and `10+`, which aren't directly numeric. If we define `< 1` as 0 and `10+` as 10, then `emp_length` can also be cast as numeric.\n",
    "\n",
    "We demonstrate the steps for converting these string variables into numeric values below.\n",
    "\n",
    "### Convert `int_rate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans[\"int_rate\"] = loans[\"int_rate\"].gsub(pattern = \"%\", replacement = \"\") # strip %\n",
    "loans[\"int_rate\"] = loans[\"int_rate\"].trim() # trim whitespace\n",
    "loans[\"int_rate\"] = loans[\"int_rate\"].asnumeric() # change to numeric "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert `revol_util`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans[\"revol_util\"] = loans[\"revol_util\"].gsub(pattern=\"%\", replacement=\"\") # strip %\n",
    "loans[\"revol_util\"] = loans[\"revol_util\"].trim() # trim whitespace\n",
    "loans[\"revol_util\"] = loans[\"revol_util\"].asnumeric() # change to numeric "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert `emp_length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use gsub to remove \" year\" and \" years\"; also translate n/a to \"\" \n",
    "loans[\"emp_length\"] = loans[\"emp_length\"].gsub(pattern=\"([ ]*+[a-zA-Z].*)|(n/a)\", replacement=\"\") \n",
    "loans[\"emp_length\"] = loans[\"emp_length\"].trim() # trim whitespace\n",
    "\n",
    "loans[\"emp_length\"] = loans[\"emp_length\"].gsub(pattern=\"< 1\", replacement=\"0\") # convert \"< 1\" to 0\n",
    "loans[\"emp_length\"] = loans[\"emp_length\"].gsub(pattern=\"10\\\\+\", replacement=\"10\") # convert \"10+\" to 10\n",
    "loans[\"emp_length\"] = loans[\"emp_length\"].asnumeric() # trim whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These steps result in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans[[\"int_rate\", \"revol_util\", \"emp_length\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Interest rate distributions\n",
    "\n",
    "Now that we have converted interest rate to numeric, we can use the `hist` function to compare the interest rate distributions for good and bad loans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "print(\"Bad Loans\")\n",
    "loans[loans[\"bad_loan\"] == \"1\", \"int_rate\"].hist()\n",
    "\n",
    "print(\"Good Loans\")\n",
    "loans[loans[\"bad_loan\"] == \"0\", \"int_rate\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the bad loan distribution contains proportionately more high interest rate loans than the distribution for good loans. Likewise, the good loan distribution contains a higher proportion of low interest rate loans than that for bad loans. It would not surprise us if interest rate were a strong predictor of loan performance.\n",
    "\n",
    ">Financial institutions typically set a borrower's interest rate based on factors like estimated risk and customer demand. If the underwriting rules are any good at all, we would expect interest rate to be one of the best predictors of default. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Clean up messy categorical columns\n",
    "\n",
    "Much as we did with the `loan_status` column, the `verification_status` column needs cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans[\"verification_status\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there are multiple values that mean verified (\"VERIFIED - income\" and \"VERIFIED - income source\"), we replace them simply with \"verified\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans[\"verification_status\"] = loans[\"verification_status\"].sub(pattern=\"VERIFIED - income source\", \n",
    "                                                                replacement=\"verified\")\n",
    "loans[\"verification_status\"] = loans[\"verification_status\"].sub(pattern=\"VERIFIED - income\", \n",
    "                                                                replacement=\"verified\")\n",
    "loans[\"verification_status\"] = loans[\"verification_status\"].asfactor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resulting in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loans[\"verification_status\"].table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 (of 10).  Feature engineering\n",
    "\n",
    "Now that we have cleaned our data, we can extract information from our current columns to create new features. This process is referred to as _feature engineering_. The general idea is to express information found in our data in a manner that is most understandable to the algorithms we employ, with the goal of improving the performance of our supervised learning models.\n",
    "\n",
    "Feature engineering can be considered the \"secret sauce\" in building a superior predictive model: it is often (although not always) more important than the choice of machine learning algorithm. A very good summary of feature engineering recipes can be found in the online [Driverless AI Documentation](http://docs.h2o.ai/driverless-ai/latest-stable/docs/userguide/transformations.html). \n",
    "\n",
    "We will do some basic feature engineering using the date fields in our data, and then use NLP (natural language processing) to create word embedding features from the loan description text field in our data.\n",
    "\n",
    "The new columns we will create are: \n",
    "* `credit_length`: the number of years someone has had a credit history\n",
    "* `issue_d_year` and `issue_d_month`: the year and month from the loan issue date\n",
    "* word embeddings from the loan description\n",
    "\n",
    "### Credit Length\n",
    "\n",
    "We create the `credit_length` feature by subtracting the year of a customer's earliest credit line from the year they were issued the loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans[\"credit_length\"] = loans[\"issue_d\"].year() - loans[\"earliest_cr_line\"].year()\n",
    "loans[\"credit_length\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue Date Expansion\n",
    "\n",
    "We next extract the year and month from the issue date.  We may find that the month or the year when the loan was issued will impact the probability of a bad loan. Additionally, since months are cyclical we will treat `issue_d_month` as a factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans[\"issue_d_year\"] = loans[\"issue_d\"].year()\n",
    "loans[\"issue_d_month\"] = loans[\"issue_d\"].month().asfactor()\n",
    "\n",
    "loans[[\"issue_d_year\", \"issue_d_month\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "\n",
    "One of the columns in our dataset is a user-provided description of why the loan was requested. The first few descriptions in the dataset are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans[\"desc\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The descriptions may contain information that would assist in predicting default, but supervised learning algorithms in general have a hard time understanding text. We need to convert these strings into a numeric representation of the text in order for our algorithms to operate on it. There are multiple choices for doing so, in this example we will use the Word2Vec algorithm.\n",
    "\n",
    "We start by defining stop words (terms that are considered too frequent to carry much information) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = [\"ax\",\"i\",\"you\",\"edu\",\"s\",\"t\",\"m\",\"subject\",\"can\",\"lines\",\"re\",\"what\",\n",
    "              \"there\",\"all\",\"we\",\"one\",\"the\",\"a\",\"an\",\"of\",\"or\",\"in\",\"for\",\"by\",\"on\",\n",
    "              \"but\",\"is\",\"in\",\"a\",\"not\",\"with\",\"as\",\"was\",\"if\",\"they\",\"are\",\"this\",\"and\",\"it\",\"have\",\n",
    "              \"from\",\"at\",\"my\",\"be\",\"by\",\"not\",\"that\",\"to\",\"from\",\"com\",\"org\",\"like\",\"likes\",\"so\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next _tokenize_ the descriptions by breaking the text into individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences, stop_word = STOP_WORDS):\n",
    "    tokenized = sentences.tokenize(\"\\\\W+\")\n",
    "    tokenized_lower = tokenized.tolower()\n",
    "    tokenized_filtered = tokenized_lower[(tokenized_lower.nchar() >= 2) | (tokenized_lower.isna()),:]\n",
    "    tokenized_words = tokenized_filtered[tokenized_filtered.grep(\"[0-9]\",invert=True,output_logical=True),:]\n",
    "    tokenized_words = tokenized_words[(tokenized_words.isna()) | (~ tokenized_words.isin(STOP_WORDS)),:]\n",
    "    return tokenized_words\n",
    "\n",
    "words = tokenize(loans[\"desc\"].ascharacter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train our Word2Vec model on the words extracted from our descriptions. We choose an output vector size of 100.\n",
    "\n",
    ">What does Word2Vec do? At a high level, it is a dimensionality reduction method for numerical representations of text. But it reduces dimensionality while preserving relationships between words in the text.\n",
    ">\n",
    ">Suppose we were to create a dictionary of all the words in our descriptions, and further suppose that dictionary contained 2500 unique words. At one extreme, we could create an indicator variable for each word (i.e., one-hot encoding). This would yield 2500 new features that would certainly lead to massive overfitting of models.\n",
    ">\n",
    ">At the other extreme, suppose we had someone classify those words into different groups and create indicator variables for each group: e.g., `risky_words` (\"bankruptcy\", \"default\", \"forfeit\", \"lien\", etc.), `angry_words` (profanity, \"complaint\", etc.), and so on. This reduces dimensionality by manually grouping words, but it is extremely labor intensive.\n",
    ">\n",
    ">Word2Vec starts with the entire dictionary size $K$ as inputs and the selected vector size $k$ as the target number of outputs. Passing through the intermediate layer(s) of the Word2Vec neural net, a $k$-dimensional numeric representation of each word is derived. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators.word2vec import H2OWord2vecEstimator\n",
    "\n",
    "w2v_model = H2OWord2vecEstimator(vec_size=100, model_id=\"w2v\")\n",
    "w2v_model.train(training_frame=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way we quickly sanity check the Word2Vec model is by finding synonyms for specified words, e.g., \"car\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.find_synonyms(\"car\", count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming we are satisfied with our Word2Vec model results, we next calculate a vector for each description by averaging over all of the words in that description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "desc_vecs = w2v_model.transform(words, aggregate_method=\"AVERAGE\")\n",
    "desc_vecs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we add the aggregated word embeddings from the Word2Vec model to the loans data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans = loans.cbind(desc_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 (of 10). Model training\n",
    "\n",
    "Now that we have cleaned our data and added new columns, we train a model to predict bad loans. First split our loans data into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = loans.split_frame(seed=25, ratios=[0.75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next create a list of predictors as a subset of the columns of the `loans` H2O Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remove = [\"initial_list_status\",\n",
    "                  \"out_prncp\",\n",
    "                  \"out_prncp_inv\",\n",
    "                  \"total_pymnt\",\n",
    "                  \"total_pymnt_inv\",\n",
    "                  \"total_rec_prncp\", \n",
    "                  \"total_rec_int\",\n",
    "                  \"total_rec_late_fee\",\n",
    "                  \"recoveries\",\n",
    "                  \"collection_recovery_fee\",\n",
    "                  \"last_pymnt_d\", \n",
    "                  \"last_pymnt_amnt\",\n",
    "                  \"next_pymnt_d\",\n",
    "                  \"last_credit_pull_d\",\n",
    "                  \"collections_12_mths_ex_med\" , \n",
    "                  \"mths_since_last_major_derog\",\n",
    "                  \"policy_code\",\n",
    "                  \"loan_status\",\n",
    "                  \"funded_amnt\",\n",
    "                  \"funded_amnt_inv\",\n",
    "                  \"mths_since_last_delinq\",\n",
    "                  \"mths_since_last_record\",\n",
    "                  \"id\",\n",
    "                  \"member_id\",\n",
    "                  \"desc\",\n",
    "                  \"zip_code\"]\n",
    "\n",
    "predictors = list(set(loans.col_names) - set(cols_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create an XGBoost model for predicting loan default. This model is being run with almost all of the model-tuning values at their defaults. Later we may want to optimize the hyperparameters using a grid search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators import H2OXGBoostEstimator\n",
    "\n",
    "param = {\n",
    "      \"ntrees\" : 20\n",
    "    , \"nfolds\" : 5\n",
    "    , \"seed\": 25\n",
    "}\n",
    "xgboost_model = H2OXGBoostEstimator(**param)\n",
    "xgboost_model.train(x = predictors,\n",
    "                    y = \"bad_loan\",\n",
    "                    training_frame=train,\n",
    "                    validation_frame=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 (of 10).  Examine model accuracy\n",
    "\n",
    "The plot below shows the performance of the model as more trees are built.  This graph can help us see at what point our model begins overfitting.  Our test data error rate stops improving at around 8-10 trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "xgboost_model.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve of the training and testing data are shown below.  The area under the ROC curve is much higher for the training data than the test data, indicating that the model is beginning to memorize the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Data\")\n",
    "xgboost_model.model_performance(train = True).plot()\n",
    "print(\"Testing Data\")\n",
    "xgboost_model.model_performance(valid = True).plot()\n",
    "print(\"X-Val\")\n",
    "xgboost_model.model_performance(xval=True).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7 (of 10). Interpret model\n",
    "\n",
    "The variable importance plot shows us which variables are most important to predicting `bad_loan`.  We can use partial dependency plots to learn more about how these variables affect the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model.varimp_plot(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suspected, interest rate appears to be the most important feature in predicting loan default. The partial dependency plot of the `int_rate` predictor shows us that as the interest rate increases, the likelihood of the loan defaulting also increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pdp = xgboost_model.partial_plot(cols=[\"int_rate\"], data=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8 (of 10). Save and reuse model\n",
    "\n",
    "The model can either be embedded into a self-contained Java MOJO package\n",
    "or it can be saved and later loaded directly into an H2O-3 cluster. For production\n",
    "use, we recommend using MOJO as it is optimized for speed. See the [guide](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/productionizing.html) for further information.\n",
    "\n",
    "### Downloading MOJO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model.download_mojo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and reuse the model \n",
    "\n",
    "We can save the model to disk for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = h2o.save_model(model=xgboost_model, force=True)\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the H2O cluster shuts down, all unsaved data and models are lost. At some future date, we can load the model for batch scoring in the H2O cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = h2o.load_model(path=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using that model, we can also score new data with the predict function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bad_loan_hat = loaded_model.predict(test)\n",
    "bad_loan_hat.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9 (of 10). AutoML (optional)\n",
    "\n",
    "AutoML can be used for automating the machine learning workflow, which includes automatic training and tuning of many models within a user-specified time-limit or user specified model build limit. \n",
    "\n",
    "Stacked Ensembles will be automatically trained on collections of individual models to produce highly predictive ensemble models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "aml = H2OAutoML(max_models=5, \n",
    "                max_runtime_secs_per_model=60, \n",
    "                include_algos = [\"GLM\", \"DRF\", \"XGBoost\", \"StackedEnsemble\"],\n",
    "                seed=25)\n",
    "aml.train(x=predictors, y='bad_loan', training_frame=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_While the AutoML job is running, you can open H2O Flow and monitor the model building process._\n",
    "\n",
    "Once complete, the leaderboard contains the performance metrics of the models generated by AutoML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "aml.leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we provided only the training H2O Frame during training, the models are sorted by their cross-validated performance metrics (AUC by default for classification). We can evaluate the best model (`leader`) on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml.leader.model_performance(test_data=test).plot()\n",
    "aml.leader.model_performance(test_data=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another convenient use of H2O Flow is to explore the various models built by AutoML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10 (of 10). Stop the Sparkling Water cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.cluster().shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your work is completed, shutting down the Sparkling Water cluster frees up the resources reserved by H2O."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
