{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<img src=\"../src/img/h2o_banner2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "heading_collapsed": true
   },
   "source": [
    "## License "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true
   },
   "source": [
    "<span style=\"color:gray\"> Copyright 2019 David Whiting and the H2O.ai team\n",
    "\n",
    "<span style=\"color:gray\"> Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "<span style=\"color:gray\">     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "<span style=\"color:gray\"> Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "<span style=\"color:gray\"> **DISCLAIMER:** This notebook is not legal compliance advice. </span>\n",
    "\n",
    "<hr style=\"background-color: gray;height: 2.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H2O.ai Lesson 2: Gradient Boosting Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the second in a series of instructional Jupyter notebooks on Sparkling Water. These notebooks are built to be run on the H2O.ai Aquarium training platform [https://aquarium.h2o.ai](https://aquarium.h2o.ai). \n",
    "\n",
    "_**Experienced modelers should find the content in these notebooks sufficient to be up-and-running in H2O Sparkling Water immediately**_. \n",
    "\n",
    "----\n",
    "\n",
    "<div style=\"margin-left: 3em;\">\n",
    "\n",
    "### Intended Audience\n",
    "\n",
    "The target audience for this training notebook is data scientists, machine learning engineers, and other experienced modelers. Technically advanced analysts may also find this training understandable. A working knowledge of Python and previous experience building statistical or machine learning models is assumed.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Successful completion of \n",
    "    \n",
    "<ul style=\"list-style: none;\">\n",
    "    <li><input type=\"checkbox\"><span style=\"color:blue\">\n",
    "        H2O.ai Lesson 1: Introduction to H2O Sparkling Water\n",
    "        </span></li>\n",
    "</ul> \n",
    "\n",
    "### Learning Outcomes\n",
    "\n",
    "By the end of this notebook, you will be able to ...\n",
    "<ul style=\"list-style: none;\">\n",
    "    <li><input type=\"checkbox\" disabled ><span style=\"color:black\">\n",
    "    Explain at a high level how Gradient Boosting Models work, and how the GBM and XGBoost algorithms differ\n",
    "    </span></li><li><input type=\"checkbox\" disabled><span style=\"color:black\">\n",
    "    Build gradient boosting predictive models with H2O GBM and H2O XGBoost algorithms\n",
    "    </span></li><li><input type=\"checkbox\" disabled><span style=\"color:black\">\n",
    "    Use H2O Flow to investigate model builds and performance \n",
    "    </span></li>\n",
    "</ul>\n",
    "\n",
    "</div>\n",
    "\n",
    "<hr style=\"background-color: black;height: 2.0px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Gradient Boosting Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Introduction\n",
    "\n",
    "These lessons assume some familiarity with statistical and machine learning models such as logistic regression, decision trees, random forests, gradient boosting models, etc. In this section, we will give a high-level overview of decision trees and gradient boosting models, concentrating on two specific implementations: H2O GBM and H2O XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Decision Trees\n",
    "\n",
    "At the heart of each GBM implementation is the concept of a **decision tree**.\n",
    "\n",
    "A decision tree can be used for either\n",
    "\n",
    "- _classification:_ assign observations to discrete groups \n",
    "- _regression:_ assign observations a predicted continuous outcome \n",
    "\n",
    "Observation assignment is made through _conditional control statements_ that form a tree-like structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td style=\"width: 550px; text-align: left; vertical-align: top;\">\n",
    "    <h2>How does it work?</h2>\n",
    "    <ul><li style=\"margin: 6px 0;\">\n",
    "        Search through all candidate predictors. Identify the variable <strong>split</strong> that yields the greatest predictive power. \n",
    "    </li><li style=\"margin: 6px 0;\">\n",
    "    For each created branch, follow the same process again.\n",
    "    </li><li style=\"margin: 6px 0;\">\n",
    "        Repeat until <strong>stopping criteria</strong> are reached.\n",
    "    </li></ul>\n",
    "    <h2>Examples of splitting functions</h2>:\n",
    "    <ul><li style=\"margin: 6px 0;\">\n",
    "    Gini function\n",
    "    </li><li style=\"margin: 6px 0;\">\n",
    "    Information entropy\n",
    "    </li></ul>\n",
    "    <h2>Examples of stopping criteria</h2>:\n",
    "    <ul><li style=\"margin: 6px 0;\">\n",
    "    Minimum number of observations needed at each node after splitting\n",
    "    </li><li style=\"margin: 6px 0;\">\n",
    "    Entropy not reduced more than some cutoff\n",
    "    </li><li style=\"margin: 6px 0;\">\n",
    "    Maximum layers of tree (i.e., depth)\n",
    "    </li></ul> \n",
    "</td>\n",
    "<td style=\"width: 500px; text-align: left;\">\n",
    "    <img src=\"../src/img/titanic_decision_tree.png\" style=\"height:400px\"></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Decision tree strengths and weaknesses</h2>\n",
    "\n",
    "<table align=left>\n",
    "<tr>\n",
    "<td style=\"width: 300px; text-align: left; vertical-align: top;\">\n",
    "    <h2>Strengths</h2>\n",
    "    <ul><li style=\"margin: 6px 0;\">\n",
    "    Simple to understand, easy to interpret\n",
    "    </li><li style=\"margin: 6px 0;\">\n",
    "    Robust to\n",
    "    </li><ul><li style=\"margin: 6px 0;\">\n",
    "    nonlinear relationships\n",
    "    </li><li style=\"margin: 6px 0;\">\n",
    "    correlated features\n",
    "    </li><li style=\"margin: 6px 0;\">\n",
    "    feature distributions\n",
    "    </li><li style=\"margin: 6px 0;\">\n",
    "    missing values\n",
    "        </li></ul><li style=\"margin: 6px 0;\">\n",
    "    Fast to train\n",
    "    </li><li style=\"margin: 6px 0;\">\n",
    "    Fast to score\n",
    "    </li></ul> \n",
    "</td>\n",
    "<td style=\"width: 300px; text-align: left; vertical-align: top;\">\n",
    "    <h2>Weaknesses</h2>\n",
    "    <ul><li style=\"margin: 6px 0;\">\n",
    "    High variance (can easily overfit)\n",
    "    </li><li style=\"margin: 6px 0;\">\n",
    "    Poor predictive accuracy\n",
    "    </li><li style=\"margin: 6px 0;\">\n",
    "    Inefficient for linear relationships\n",
    "    </li></ul> \n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is an ensemble method that combines models sequentially, where each model is built on the residuals of a previous model\n",
    "\n",
    "### Boosted trees\n",
    "\n",
    "- Start by building a relatively shallow decision tree\n",
    "- Sequentially build the next tree based on residuals from the previous tree\n",
    "- The objective is to take an initial weak learner and gradually turn it into a strong learner by concentrating on where the model is not predicting well\n",
    "\n",
    "### Be careful:\n",
    "\n",
    "- In theory, GBM will continue to add to the number of trees to fit all the noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting implementations in H2O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For all random forest and boosting implementations, tree building is parallelized in H2O:\n",
    "  - Each tree is built in parallel.\n",
    "  - Categoricals can be split into groups (instead of just using Boolean splits).\n",
    "  - Shared histograms calculate cut-points.\n",
    "  - Greedy search of histogram bins, optimizing squared error.\n",
    "\n",
    "<img src=\"../src/img/GBM_in_H2O.png\" style=\"height:400px\">\n",
    "\n",
    "### XGBoost\n",
    "\n",
    "XGBoost is very similar to GBM with the following modification:\n",
    "\n",
    "- XGBoost employs a penalty term for the number of variables.\n",
    "- That is, it contains regularization terms in the cost function. \n",
    "- Hence, trees are grown in **breadth** instead of **depth**.\n",
    "\n",
    "### LightGBM\n",
    "\n",
    "LightGBM builds trees as deep as necessary by repeatedly splitting the one leaf that gives the biggest gain\n",
    "\n",
    "- Trees are grown in **depth** instead of **breadth**\n",
    "- In theory, LightGBM is optimized for sparse data\n",
    "\n",
    "(H2O does not implement LightGBM directly, but instead provides a method for emulating the approach using a certain set of options within XGBoost.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All boosting methods require the following hyperparameters:\n",
    "  - Number of trees to be built\n",
    "  - Shrinkage parameter (specifies rate at which model learns)\n",
    "  - Depth of the boosting tree\n",
    "\n",
    "- Note that simply adding trees in boosting approaches (without further restrictions) can lead to overfitting\n",
    "\n",
    "- A grid search can aid the process of hyperparameter selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><span style=\"color:black\">\n",
    "\n",
    "## Completed Learning Outcomes\n",
    "\n",
    "<ul style=\"list-style: none;\">\n",
    "    <li><input type=\"checkbox\" disabled checked>\n",
    "    Explain at a high level how Gradient Boosting Models work, and how the GBM and XGBoost algorithms differ\n",
    "    </li>\n",
    "</ul>\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we set up the Sparkling Water environment and perform all of the preliminary data tasks that we developed in H2O Lesson 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment preparation\n",
    "\n",
    "In Lesson 1, we used the `spark` command to initiate a `SparkSession`. Because of the way we have configured the notebook `PySparkling` kernel, a `SparkSession` is actually created during startup of the Jupyter notebook. In other words, the `spark` command is not needed on this platform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysparkling import *\n",
    "hc = H2OContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><span style=\"color:black\">\n",
    "Remember that how you start SparkSession and Sparkling Water will depend on your specific installation setup.\n",
    "</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "import os\n",
    "input_csv = \"/home/h2o/data/lending_club/LoanStats3a.csv\"\n",
    "\n",
    "if not os.path.exists(input_csv):\n",
    "    input_csv = \"https://s3-us-west-2.amazonaws.com/h2o-tutorials/data/topics/lending/lending_club/LoanStats3a.csv\"\n",
    "\n",
    "loans = h2o.import_file(input_csv,\n",
    "                        col_types = {\"int_rate\":\"string\", \n",
    "                                     \"revol_util\":\"string\", \n",
    "                                     \"emp_length\":\"string\", \n",
    "                                     \"verification_status\":\"string\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Data munging and feature engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><span style=\"color:black\">\n",
    "Please reference Lesson 1 for details on how and why we performed these steps.\n",
    "</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ongoing_status = [\"Current\",\n",
    "                  \"In Grace Period\",\n",
    "                  \"Late (16-30 days)\",\n",
    "                  \"Late (31-120 days)\",\n",
    "                  \"Does not meet the credit policy.  Status:Current\",\n",
    "                  \"Does not meet the credit policy.  Status:In Grace Period\"]\n",
    "loans = loans[~loans[\"loan_status\"].isin(ongoing_status)]\n",
    "\n",
    "response = \"bad_loan\"\n",
    "fully_paid = [\"Fully Paid\",\n",
    "              \"Does not meet the credit policy.  Status:Fully Paid\"]\n",
    "loans[response] = ~(loans[\"loan_status\"].isin(fully_paid))\n",
    "loans[response] = loans[response].asfactor()\n",
    "\n",
    "loans[\"int_rate\"] = loans[\"int_rate\"].gsub(pattern = \"%\", replacement = \"\") # strip %\n",
    "loans[\"int_rate\"] = loans[\"int_rate\"].trim() # trim whitespace\n",
    "loans[\"int_rate\"] = loans[\"int_rate\"].asnumeric() # change to numeric \n",
    "\n",
    "loans[\"revol_util\"] = loans[\"revol_util\"].gsub(pattern=\"%\", replacement=\"\")\n",
    "loans[\"revol_util\"] = loans[\"revol_util\"].trim()\n",
    "loans[\"revol_util\"] = loans[\"revol_util\"].asnumeric()\n",
    "\n",
    "loans[\"emp_length\"] = loans[\"emp_length\"].gsub(pattern=\"([ ]*+[a-zA-Z].*)|(n/a)\", replacement=\"\") \n",
    "loans[\"emp_length\"] = loans[\"emp_length\"].trim()\n",
    "loans[\"emp_length\"] = loans[\"emp_length\"].gsub(pattern=\"< 1\", replacement=\"0\") # convert \"< 1\" to 0\n",
    "loans[\"emp_length\"] = loans[\"emp_length\"].gsub(pattern=\"10\\\\+\", replacement=\"10\") # convert \"10+\" to 10\n",
    "loans[\"emp_length\"] = loans[\"emp_length\"].asnumeric()\n",
    "\n",
    "loans[\"verification_status\"] = loans[\"verification_status\"].sub(pattern=\"VERIFIED - income source\", \n",
    "                                                                replacement=\"verified\")\n",
    "loans[\"verification_status\"] = loans[\"verification_status\"].sub(pattern=\"VERIFIED - income\", \n",
    "                                                                replacement=\"verified\")\n",
    "loans[\"verification_status\"] = loans[\"verification_status\"].asfactor()\n",
    "\n",
    "loans[\"credit_length\"] = loans[\"issue_d\"].year() - loans[\"earliest_cr_line\"].year()\n",
    "loans[\"issue_d_year\"] = loans[\"issue_d\"].year()\n",
    "loans[\"issue_d_month\"] = loans[\"issue_d\"].month().asfactor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Create the predictor list used in modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have not excluded `int_rate` from the list of predictors, as you did in the Lesson 1 assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remove = [\"initial_list_status\",\n",
    "                  \"out_prncp\",\n",
    "                  \"out_prncp_inv\",\n",
    "                  \"total_pymnt\",\n",
    "                  \"total_pymnt_inv\",\n",
    "                  \"total_rec_prncp\", \n",
    "                  \"total_rec_int\",\n",
    "                  \"total_rec_late_fee\",\n",
    "                  \"recoveries\",\n",
    "                  \"issue_d\",\n",
    "                  \"collection_recovery_fee\",\n",
    "                  \"last_pymnt_d\", \n",
    "                  \"last_pymnt_amnt\",\n",
    "                  \"next_pymnt_d\",\n",
    "                  \"last_credit_pull_d\",\n",
    "                  \"collections_12_mths_ex_med\" , \n",
    "                  \"mths_since_last_major_derog\",\n",
    "                  \"policy_code\",\n",
    "                  \"loan_status\",\n",
    "                  \"funded_amnt\",\n",
    "                  \"funded_amnt_inv\",\n",
    "                  \"mths_since_last_delinq\",\n",
    "                  \"mths_since_last_record\",\n",
    "                  \"id\",\n",
    "                  \"member_id\",\n",
    "                  \"desc\",\n",
    "                  \"zip_code\"]\n",
    "\n",
    "predictors = list(set(loans.col_names) - set(cols_to_remove))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"alert alert-block alert-success\"><span style=\"color:black\">\n",
    "    <strong>Recall</strong>: Explain why we might want to include or exclude interest rate in a model. What are the different possible use cases?\n",
    "</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\"><span style=\"color:black\">\n",
    "\n",
    "If our loan underwriting is any good at all, then interest rate should be one of the most important predictors of default risk, since the interest rate a customer is offered is based in large part on risk estimation. (Customer demand may also come into play, especially if we are optimizing price.) Therefore ...\n",
    "\n",
    "**Exclude interest rate**: when we are building a risk model and/or determining interest rate from the risk profile;\n",
    "\n",
    "**Include interest rate**: when we are monitoring the effect of our pricing model, when we want to use it as an offset and see what other variables are important or might have been missed in building our risk model, etc.\n",
    "\n",
    "</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Baseline Model Building: GBM and XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Splitting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data into a training, validation, and testing set is the accepted standard for model building when your data size is sufficiently large. Alternatively, we can split data into 80% training and 20% test sets and use k-fold cross-validation on the training data. This is computationally more expensive but allows the model to see more data in training. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><span style=\"color:black\">\n",
    "The definition of \"sufficiently large\" is data and problem specific. We will demonstrate both approaches.  \n",
    "    </span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional train, validate, and test set splits\n",
    "\n",
    "We split the data into three parts: 60% for training, 20% for validation, and 20% for final testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = loans.split_frame(seed = 12345,\n",
    "                                       ratios = [0.6, 0.2],\n",
    "                                       destination_frames = ['train.hex', 'valid.hex', 'test.hex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test split for cross-validation\n",
    "\n",
    "We can also split the data into two parts: 80% for training and 20% for final testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cv, test_cv = loans.split_frame(seed = 12345,\n",
    "                                      ratios = [0.8],\n",
    "                                      destination_frames = ['train_cv.hex', 'test_cv.hex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Baseline GBM train-validate-test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model we fit is a default GBM, trained on the 60% training split with default settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "\n",
    "gbm = H2OGradientBoostingEstimator(seed = 12345)\n",
    "gbm.train(x = predictors,\n",
    "          y = response,\n",
    "          training_frame = train,\n",
    "          validation_frame = valid,\n",
    "          model_id = \"gbm_baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below shows the performance of the model as more trees are built.  This graph can help us see at what point our model begins overfitting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "gbm.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data error rate stops improving at around 10-15 trees.\n",
    "\n",
    "We can get a detailed model summary using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBM model performance\n",
    "\n",
    "Let's visualize the performance of the default GBM model across all splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train\")\n",
    "gbm.model_performance(train).plot()\n",
    "\n",
    "print(\"Validation\")\n",
    "gbm.model_performance(valid).plot()\n",
    "\n",
    "print(\"Test\")\n",
    "gbm.model_performance(test).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the AUC on the validation data split, we enter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gbm.model_performance(valid).auc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results confirm what we saw in the scoring history plot: the GBM model is significantly overfit on the training set. (This is not unexpected.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBM model interpretation\n",
    "\n",
    "The variable importance plot shows us which variables are most important to predicting `bad_loan`.  We can use partial dependence plots to learn more about how these variables affect the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm.varimp_plot(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial dependence plot of the `int_rate` predictor shows us that as the interest rate increases, the likelihood of the loan defaulting also increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp = gbm.partial_plot(cols=[\"int_rate\"], data=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One-dimensional partial dependence plots (PDPs) show us the average behavior of a complex response function with respect to a single input\n",
    "* They allow us to compare this average behavior to domain knowledge and expected behavior\n",
    "* **Note: _The average behavior of PDPs can be misleading in the presence of strong interactions or for highly nonlinear response functions_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Baseline XGBoost cross-validated model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a baseline XGBoost model using 5-fold cross-validation and the train/test data split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators import H2OXGBoostEstimator\n",
    "\n",
    "xgb_cv = H2OXGBoostEstimator(nfolds = 5, seed = 12345)\n",
    "xgb_cv.train(x = predictors,\n",
    "             y = response,\n",
    "             training_frame = train_cv,\n",
    "             validation_frame = test_cv,\n",
    "             model_id = \"xgb_cv_baseline\"\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the scoring history using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_cv.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our error rate stops improving at around 10 trees.\n",
    "\n",
    "We can get a detailed model summary using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgb_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost model performance\n",
    "\n",
    "The model performance metrics are given in a similar manner below. Note the slight difference in syntax we use here for cross-validated models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train\")\n",
    "xgb_cv.model_performance(train = True).plot()\n",
    "\n",
    "print(\"Cross-Validation\")\n",
    "xgb_cv.model_performance(xval = True).plot()\n",
    "\n",
    "print(\"Test\")\n",
    "xgb_cv.model_performance(valid = True).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the AUC on the cross-validated data, we enter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgb_cv.model_performance(xval = True).auc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the GBM model, the results above indicate that the baseline XGBoost model is significantly overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost model interpretation\n",
    "\n",
    "The variable importance plot shows us which variables are most important to predicting `bad_loan`.  We can use partial dependence plots to learn more about how these variables affect the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_cv.varimp_plot(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial dependence plot of the `int_rate` predictor shows us that as the interest rate increases, the likelihood of the loan defaulting also increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp = xgb_cv.partial_plot(cols=[\"int_rate\"], data=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><span style=\"color:black\">\n",
    "    \n",
    "A comparison of the variable importance plots for GBM and XGBoost demonstrates some of the differences in these two algorithms. For instance, the H2O GBM implementation can handle high-cardinality categorical variables (e.g., `addr_state`) directly, while XGBoost opts for one-hot encoding. A choice between algorithms often comes down to performance.\n",
    "    \n",
    "In either case, we could opt to build models after target-encoding high-cardinality categorical variables. We save such feature engineering approaches for a future lesson.    \n",
    "</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><span style=\"color:black\">\n",
    "\n",
    "## Completed Learning Outcomes\n",
    "\n",
    "<ul style=\"list-style: none;\">\n",
    "    <li><input type=\"checkbox\" disabled checked><span style=\"color:gray\">\n",
    "    Explain at a high level how Gradient Boosting Models work, and how the GBM and XGBoost algorithms differ\n",
    "        </span><li><input type=\"checkbox\" disabled checked>\n",
    "        Build gradient boosting predictive models with H2O GBM and H2O XGBoost algorithms\n",
    "    </li>\n",
    "</ul>\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Using H2O Flow for model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `Models` directory to list all models, or input directly using the `getModels` command. Your results should look something like\n",
    "\n",
    "<img src=\"../src/img/flow_get_models_new.png\" style=\"height:400px\">\n",
    "\n",
    "Note that this contains the GBM baseline model, the XGBoost baseline model, and the five XGBoost folds from our cross-validation. Clicking on a model name brings up\n",
    "\n",
    "<img src=\"../src/img/flow_logloss_new.png\" style=\"height:600px\">\n",
    "\n",
    "and other performance information such as variable importance\n",
    "\n",
    "<img src=\"../src/img/flow_importance_new.png\" style=\"height:600px\">\n",
    "\n",
    "Make sure you can find at very least\n",
    "\n",
    "- Scoring history plots\n",
    "- AUC metrics and ROC plots\n",
    "- Variable importances\n",
    "- Confusion matrices\n",
    "- Model parameters\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><span style=\"color:black\">\n",
    "H2O Flow is a very convenient tool for interactive model investigation.\n",
    "</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><span style=\"color:black\">\n",
    "    \n",
    "### YOUR TURN: Investigate the cross-validated XGBoost model using H2O Flow.\n",
    "\n",
    "Find\n",
    "\n",
    "- Scoring history plots\n",
    "- AUC metrics and ROC plots\n",
    "- Variable importances\n",
    "- Confusion matrices\n",
    "- XGBoost parameters\n",
    "\n",
    "</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><span style=\"color:black\">\n",
    "\n",
    "## Completed Learning Outcomes\n",
    "\n",
    "<ul style=\"list-style: none;\">\n",
    "    <li><input type=\"checkbox\" disabled checked><span style=\"color:gray\">\n",
    "    Explain at a high level how Gradient Boosting Models work, and how the GBM and XGBoost algorithms differ\n",
    "    </span><li><input type=\"checkbox\" disabled checked><span style=\"color:gray\">\n",
    "    Build gradient boosting predictive models with H2O GBM and H2O XGBoost algorithms\n",
    "    </span><li><input type=\"checkbox\" disabled checked>    \n",
    "    Use H2O Flow to investigate model builds and performance \n",
    "    </li>\n",
    "</ul>\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><span style=\"color:black\">\n",
    "    \n",
    "## Part 1. Build and evaluate the following models:\n",
    "\n",
    "###  1.1. Baseline GBM using 5-fold cross-validation \n",
    "    \n",
    "- Name the model \"gbm_cv\"\n",
    "- Add as many cells below as needed to complete.\n",
    "\n",
    "    </span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline GBM cross-validated model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><span style=\"color:black\">\n",
    "    \n",
    "### 1.2. Baseline XGBoost model using the 60% train, 20% validate, 20% test data\n",
    "    \n",
    "- Name the model \"xgb\"\n",
    "- Add as many cells below as needed to complete.\n",
    "    \n",
    "</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline XGBoost train-validate-test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><span style=\"color:black\">\n",
    "    \n",
    "## Part 2. Compare baseline cross-validated models\n",
    "\n",
    "Compare the **performance** of the baseline cross-validated XGBoost model with the baseline cross-validated GBM model.\n",
    "\n",
    "- Insert as many cells below as needed to complete.\n",
    "</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBM vs. XGBoost cross-validated model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><span style=\"color:black\">\n",
    "    \n",
    "## Part 3. Use H2O Flow to compare variable importance\n",
    "    \n",
    "- What are the top 5 variables in the \"gbm\" model?\n",
    "- What are the top 5 variables in the \"xgb\" model?\n",
    "\n",
    "</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Shut down the Sparkling Water Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.cluster().shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your work is completed, shutting down the H2O cluster frees up the resources reserved by H2O."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CONGRATULATIONS! You have completed Lesson 2.</h1>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySparkling",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
